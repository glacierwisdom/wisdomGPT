# LLM for CARE：意图识别论文集

## 目录
- [引言](#引言)
- [页面内容](#页面内容)
- [参考文献](#参考文献)
- [图片](#图片)

## 引言
本文档提取自 意图识别论文集，包含相关理论框架或论文整理信息。

## 页面内容

### 第 1 页
意图识别论⽂集整理信息 1. \* AMultimodalArtificialIntelligenceModelforDepressionSeverityDetectionBasedon AudioandVideoSignals.pdf 《AMultimodalArtificialIntelligenceModelforDepressionSeverityDetectionBasedonAudioandVideoSignals》提出 ⼀种结合⾳频和视频信号的轻量级、⾼精度（区分不同程度）的多模态抑郁严重程度检测模型。 在这项研究中，设计并实现了⼀个基于⻓短期记忆⽹络（LSTM）的多模态抑郁严重程度识别系统。通过改进数据预处理流程，包括 主题分割、处理不同主题的数据集以及处理伪装表情，优化后的LSTM⽹络有效降低了过拟合的⻛险，并显著增强了模型的泛化能 ⼒。利⽤多模态融合技术，我们成功地减⼩了模型的体积，同时实现了有效的特征提取。在公共和私有数据集上的实验结果表明， 该模型在轻量级设计和实时准确检测⽅⾯表现出⾊。实验结果表明，改进的⼈⼯智能模型在抑郁症严重程度识别中达到了83.86%的 准确率。 • 多模态融合：结合⾳频和视频信号，优化特征提取和融合策略。 • 轻量化设计：通过模型压缩和优化，实现⾼效部署。 • 抑郁严重程度评估：不仅检测抑郁存在与否，还量化其严重程度，为临床诊断提供⽀持。 数据集：ExtendedDistressAnalysisInterviewCorpus(E-DAIC)是⼀个扩展的数据集，旨在⽀持焦虑、抑郁和创伤后应激障碍等⼼ 理困扰的诊断。它包含半临床访谈，旨在开发⼀种能够采访个⼈并识别精神疾病⾔语和⾮⾔语指标的计算机代理。 2. \* annurev-control-071223-105834.pdf综述 《InferringHumanIntentandPredictingHumanActioninHuman‒RobotCollaboration》系统回顾了⼈机协作中意图推断 和动作预测的研究，涵盖了⻉叶斯⽅法、⻢尔可夫模型、监督学习、神经⽹络等多种技术，讨论了推断信任、能⼒、疲劳等协作特 征的⽅法，以及确定性和概率性动作预测⽅法。 论⽂聚焦于近三年的研究，将相关研究分为三⼤类（⻅图1）： 推断意图和⽬标（InferringIntentionsandGoals）：关注⼈类⾏为背后的不可观测⽬标，例如整体策略、具体活动⽬标或空间⽬ 标。 推断协作特征（InferringCollaborativeFeatures）：关注与协作相关的特定⼈类特征，如疲劳、压⼒、能⼒或对机器⼈的信任。 预测⼈类动作（PredictingHumanMotion）：关注⼈类的可观测物理运动，如轨迹或位置预测。 这些类别并⾮完全独⽴，意图推断和动作预测之间存在模糊边界。例如，某些⽅法结合了⽬标推断和运动预测（如Leetal.使⽤逆 强化学习和循环神经⽹络同时推断⽬标和轨迹）。 推断意图和⽬标 本节讨论机器⼈如何推断⼈类的意图或⽬标，包括空间⽬标（如⼿的⽬标位置）、协作策略（如轮流操作）或选择特定⽬标。 ⻉叶斯⽅法 ⻉叶斯⽅法通过计算给定观测的意图后验概率进⾏推断，适⽤于共享⾃主性场景（如辅助机器⼈）。例如： • Jain&Argall(2019)：使⽤⻉叶斯滤波框架，结合⽬标接近度和⾮语⾔动作，推断辅助任务中的⽬标。 • Ireguietal.(2021)：通过眼动追踪或触摸屏输⼊，基于多变量⾼斯分布推断⽬标物体位置。 • Felipetal.(2022)：在拾取放置任务中，使⽤近似⻉叶斯计算，通过⼈体运动学模型⽣成⼿部轨迹似然，推断⽬标位置。 ⻉叶斯⽅法的优点是计算效率⾼且结果易解释，但依赖于先验分布假设，且通常仅适⽤于瞬时推断。


### 第 2 页
⻢尔可夫模型和⻢尔可夫决策过程（MDP） ⻢尔可夫模型（如HMM、MDP、POMDP）适⽤于序列决策任务，如推断装配序列。它们通过状态转移概率建模⼈类意图，并结合 机器⼈⾏动选择。例如： • Hoffman&Breazeal(2007)：使⽤⼀阶⻢尔可夫过程建模⼯作台状态，预测⼈类下⼀动作。 • Crameretal.(2021)：使⽤POMDP建模装配路径，基于⼈类物体选择更新信念。 • Zhaoetal.(2022)：在烹饪任务中结合HMM和MDP，推断⾼层次策略。 • Nikolaidis&Shah(2013)：通过交叉训练（⼈类与机器⼈⻆⾊互换）学习MDP的奖励和转移函数。 MDP和POMDP的优势在于能同时推断意图和优化机器⼈策略，但依赖于奖励函数的准确性，常通过逆强化学习（IRL）或⼈类⽰范 学习奖励。 监督学习和回归模型 监督学习直接从⼈类活动数据映射到⽬标，⽆需显式概率模型。例如： • Tsitosetal.(2022)：使⽤线性回归和⽀持向量机，基于⼿腕追踪数据预测⽬标位置，优于朴素⻉叶斯和决策树。 • Qiaoetal.(2021)：使⽤动态运动原语（DMP）预测共享控制任务中的⽬标位置。 • Lyetal.(2021)：使⽤概率运动原语（ProMP）建模轨迹分布，捕捉运动变化。 监督学习的优点是能建模复杂⾮线性关系，但依赖⾼质量训练数据，且特征选择对结果影响较⼤。 神经⽹络模型 神经⽹络因其强⼤的映射能⼒在近年来⼴受欢迎。例如： • Choietal.(2022)：使⽤循环神经⽹络（RNN），基于⼿臂姿势和视线交点预测物体放置位置。 • Urkmez&Bozna(2022)：使⽤卷积神经⽹络（CNN）从RGB数据检测⼿势，推断指向⽬标。 • Zhuangetal.(2022)：使⽤3D卷积⽹络预测IKEA装配任务中的⼈类动作。 神经⽹络能处理复杂数据，但需要⼤量训练数据，且模型复杂性可能导致结果难以解释。 推断协作特征 本节讨论推断与协作相关的⼈类特征，如信任、能⼒、疲劳等，以优化机器⼈决策。 对机器⼈的信任 信任影响⼈类⼲预机器⼈的频率。研究者通过动态概率模型估计信任： • Xu&Dudek(2015)：使⽤动态⻉叶斯⽹络，基于⼈类⼲预和机器⼈错误估计信任。 • Chenetal.(2020)：使⽤POMDP建模信任演变，结合机器⼈性能和⼈类动作。 • Guoetal.(2021)：使⽤⻉塔分布建模信任动态，基于机器⼈性能调整参数。 信任模型可⽤于信任修复或优化协作策略。 能⼒ 推断⼈类能⼒（如专⻓、可⽤性）有助于预测⾏为和任务分配： • Carreno-Medranoetal.(2023)：通过⻉叶斯推断和轨迹分析估计⼈类专⻓。 • Liuetal.(2021)：使⽤指数函数和扩展卡尔曼滤波建模学习曲线，优化任务分配。 • Nanavatietal.(2021)：使⽤线性混合模型预测旁观者的帮助意愿，结合POMDP规划。 ⼯作量、疲劳和挫折感 推断负⾯特征可提升协作效率： • Messerietal.(2022)：使⽤深度神经⽹络从运动数据预测肌⾁激活，动态分配任务以减少疲劳。 • Lagomarsinoetal.(2022)：通过⼼电图监测⼼率间期，估计认知⼯作量，调整机器⼈轨迹。 • Mohamedetal.(2022)：结合热成像和RGB图像，使⽤k近邻算法推断⽤⼾挫折感。


### 第 3 页
预测⼈类动作 本节关注预测⼈类物理运动，分为确定性预测和概率预测两类。 确定性预测 确定性⽅法基于过去位置数据预测未来运动： • Nguyen&Xie(2021)：使⽤五参数逻辑回归模型预测指尖轨迹。 • Ericksonetal.(2023)：使⽤全连接神经⽹络，基于电容传感器数据推断肢体姿势。 • Wanetal.(2022)：使⽤图卷积神经⽹络，结合⼈类运动和物体特征预测动作。 循环神经⽹络（RNN）也被⼴泛应⽤，如Alahietal.(2016)的社会LSTM模型，⽤于多⼈运动预测。 概率预测 概率⽅法显式建模预测的不确定性： • Changetal.(2021)：使⽤卡尔曼滤波估计⼈类姿势，结合⾼斯协⽅差矩阵表⽰不确定性。 • Lietal.(2021)：使⽤⾼斯过程（GP）预测辅助穿⾐任务中的运动，确保安全性。 • Vianelloetal.(2021)：使⽤GP建模⼈体⻣骼雅可⽐零空间分布，预测协作操作中的姿势。 3.  s41562-024-01882-z.pdf 《Testingtheoryofmindinlargelanguagemodelsandhumans》研究了⼤型语⾔模型（LLMs）与⼈类在⼼智理论 （TheoryofMind,ToM）任务中的表现，探讨了这些模型是否能展现出与⼈类相似的⾏为。 摘要：⼼智理论是⼈类理解他⼈⼼理状态（如信念、意图、情感）的能⼒，是社会互动、沟通和同理⼼的核⼼。最近⼤型语⾔模型 （如ChatGPT）的发展引发了关于这些模型是否在⼼智理论任务中表现出与⼈类⾏为⽆法区分的可能性的激烈争论。在这⾥，我们 将⼈类和⼤型语⾔模型在⼀系列全⾯测量指标上的表现进⾏⽐较，旨在衡量不同的⼼智理论能⼒，从理解错误信念到解读间接请求 以及识别讽刺和失礼。我们反复测试了两类⼤型语⾔模型（GPT和LLaMA2），并将其表现与1,907名⼈类参与者的样本进⾏了⽐ 较。在⼀系列⼼智理论测试中，我们发现GPT-4模型在识别间接请求、错误信念和误导⽅⾯表现得与⼈类⽔平相当，甚⾄有时超过⼈ 类⽔平，但在检测失礼⽅⾯表现不佳。然⽽，唯⼀让LLaMA2表现优于⼈类的测试是错误判断。后续对信念可能性的操作表明， LLaMA2的优势只是表象，可能反映了归因于⽆知的偏⻅。相⽐之下，GPT的不佳表现源于过于保守地坚持结论，⽽⾮真正的推理失 败。这些发现不仅证明了⼤语⾔模型的⾏为与⼈类⼼理推理的输出⼀致，还强调了系统测试的重要性，以确保⼈类与⼈⼯智能之间 的⽐较不流于表⾯。 LLMs的⼼智推理能⼒：GPT-4在多种⼼智理论任务中表现出与⼈类⼼智推理⼀致的⾏为，表明其具有模拟⼈类⼼理状态推理的潜 ⼒。然⽽，其在社交失礼任务中的失败源于决策策略的超保守性，⽽⾮推理能⼒的根本缺陷。 LLaMA2的局限性：LLaMA2在社交失礼任务中的表现受训练数据偏⻅驱动，缺乏真正的推理能⼒。 4. \* 2504.07597v1.pdf 《LearningLongShort-TermIntentionwithinHumanDailyBehaviors》提出了⼀种新的意图预测任务和模型，通过分层建模 和冲突检测机制，使机器⼈能够更全⾯地理解⼈类⾏为，并在意图冲突时主动⼲预。 摘要：在⾃主家⽤机器⼈的领域中，机器⼈理解⼈类⾏为并提供适当服务⾄关重要。这要求机器⼈具备分析复杂⼈类⾏为和预测⼈ 类真实意图的能⼒。传统上，⼈类被视为完美⽆缺，其决策被视为机器⼈应努⼒遵循的标准。然⽽，这引发了⼀个重要问题：如果 ⼈类犯错怎么办？在这项研究中，我们提出了⼀项独特的任务，称为“⻓期短期意图预测”。该任务要求机器⼈能够预测与⼈类价 值观⼀致的⻓期意图，以及反映即时⾏动意图的短期意图。同时，机器⼈需要检测短期和⻓期意图之间的潜在不⼀致性，并提供必 要的警告和建议。为了便于完成这⼀任务，我们提出了⼀种⻓短期意图模型来表⽰复杂的意图状态，并构建了⼀个数据集来训练该 意图模型。然后，我们提出了⼀种两阶段的⽅法来整合机器⼈的意图模型：i)预测⼈类的基于价值的⻓期意图和基于⾏动的短期意 图；2)分析⻓期意图与短期意图之间的⼀致性。实验结果表明，所提出的⻓短期意图模型能够帮助机器⼈理解⼈类在⻓期和短期时 间跨度内的⾏为模式，这有助于确定⼈类⻓期意图与短期意图之间的⼀致性。 • 测试数据：包含ChatGPT⽣成的模拟数据（P01-P04）和真实⼈类⾏为数据（P05-P08）。 • 性能对⽐：


### 第 4 页
◦ 提出的模型在动作、时⻓、意图预测上均优于端到端基线模型。 ◦ ⻓期意图预测的Top-5准确率超过80%，验证了模型的有效性 5.  \_CultureandSocialBehavior-AModelfortheDevelopmentofSocialBehavior.pdf 《CultureandSocialBehavior:AModelfortheDevelopmentofSocialBehavior》提出了⼀个跨⽂化的理论模型，⽤于解释社会 ⾏为（尤其是⽇常⼈际互动⾏为）的发展过程，强调⽂化背景、社会环境和个体经历对社会⾏为塑造的影响。 • ⽇常社会⾏为的学习主要通过环境中的互动完成，⽽⾮早期童年经验的直接结果。 • ⾏为的泛化（从⼀种关系转移到另⼀种关系）是可能的，但需要进⼀步研究。 • ⽂化通过定义社会环境和活动类型，间接塑造了个体的⾏为模式。 • 性别差异（如⼥性更倾向于养育⾏为）可以通过不同性别分配的环境任务解释（如⼥孩更多被安排照顾婴⼉）。 6.  DeepNeuralNetwork-BasedModelingofMultimodalHuman‒ComputerInteractionin AircraftCockpits.pdf 基于深度学习技术提出了⼀种融合视线追踪与脑电波⽬标选择的多模态交互模型。该模型由⽬标分类与意图识别两部分构成：通过 结合操作者的眼动信息，我们构建并训练了基于⻓短期记忆⽹络的⽬标分类模型。基于变压器的意图识别模型通过结合操作者的脑 电图信息构建并训练。在⻜机雷达⻚⾯系统应⽤场景中，⽬标分类模型的最⾼准确率为98%。通过训练32通道脑电图信息获得的意 图识别率为98.5%，⾼于其他对⽐模型。此外，我们在模拟⻜⾏平台上验证了该模型，实验结果表明，所提出的多模态交互框架在 性能上优于单注视交互。 7.  IntentionRecognitionforMultipleAUVsinaCollaborativeSearchMission.pdf 摘要：本⽂探讨了协作⾃主⽔下航⾏器（AUV）搜索任务中意图识别的挑战，在这些任务中，多艘AUV必须在环境不确定性和通信 限制的情况下有效协调。我们提出了⼀种基于共识的意图识别（CBIR）⽅法，该⽅法以信念-欲望-意向（BDI）框架为基础。CBIR ⽅法结合了模糊推理和深度学习技术，以最少的数据交换预测AUV的意图，提⾼了协作决策的鲁棒性和效率。系统使⽤⾏为建模阶 段将状态特征映射到动作，并采⽤基于深度学习的意图推理阶段，利⽤残差卷积神经⽹络（ResCNN）进⾏准确的意图预测。实验结 果表明意图识别准确率：ResCNN达到95.83%，优于其他模型（CNN94.98%，LSTM94.55%）。协作搜索效率：在良好通信条件 下，CBIR⽅法将任务完成时间缩短25.6%。在极端通信条件下（如稀疏拓扑），传统⽅法失败，⽽CBIR仍能成功完成任务。 提出了⼀种基于共识的意图识别⽅法（Consensus-BasedIntentRecognition,CBIR），结合了模糊推理和深度学习技术，分为两 个阶段： 1. ⾏为建模阶段： ◦ 基于BDI（Belief-Desire-Intention）框架，将AUV的意图分为三类：优化通信（CommunicationPriority）、增强检测 （DetectionPriority）、快速接近⽬标（PositioningPriority）。 ◦ 引⼊“地标”（Landmarks）概念，表⽰任务中的关键状态，简化意图映射。 ◦ 使⽤模糊推理建⽴状态特征（如距离、通信成功率）与意图的映射关系，减少数据交换量。 2. 意图推断阶段： ◦ 采⽤残差卷积神经⽹络（ResCNN）从⾏为数据中预测意图，解决通信延迟和数据丢失问题。 ◦ ResCNN通过残差连接避免梯度消失，提升深层⽹络的训练效果。 8. \* AttentionMesh-High-fidelityFaceMeshPredictioninReal-time.pdf 由GoogleResearch团队发表，主要介绍了⼀种轻量级的3D⼈脸⽹格预测架构，能够在移动设备上实现实时⾼精度的⼈脸关键点检 测。 摘要：我们提出了⼀种轻量级架构⸺注意⼒⽹格，⽤于3D⾯部⽹格预测，该架构利⽤注意⼒机制关注具有语义意义的区域。我们 的神经⽹络专为实时设备推理设计，在Pixel2⼿机上运⾏速度超过50帧每秒。我们的解决⽅案⽀持AR化妆、眼动追踪和AR⽊偶戏等 应⽤，这些应⽤依赖于⾼度准确的眼部和唇部地标。我们的主要贡献在于统⼀的⽹络架构，实现了与多阶段级联⽅法相同的⾯部地 标精度，同时速度提⾼了30%。 • 提出统⼀模型架构，通过注意⼒机制实现⾼精度与低延迟的平衡。


### 第 5 页
9.  EnhancingDrivingControlviaSpeechRecognitionUtilizingInfluentialParametersinDeep LearningTechniques.pdf 本研究探讨了通过深度神经⽹络（DNN）利⽤语⾳识别技术增强⾃动驾驶和命令控制的⽅法。该⽅法依赖于噪声去除、从⾳频⽂件 中提取特征以及使⽤神经⽹络进⾏分类等连续阶段。在所提出的⽅法中，提取影响隐藏层结果的变量并存储在⼀个向量中，以对其 进⾏分类并找出最具影响⼒的变量，反馈到神经⽹络的隐藏层以提⾼结果的准确性。结果显⽰准确率为93%，响应时间为0.75秒， 峰值信噪⽐（PSNR）为78分⻉。 10. \* 基于注意⼒模态融合的多模态意图识别\_苏建华.pdf 摘要：意图识别是⾃然语⾔理解中的⼀项重要任务。既往关于意图识别的研究主要集中于针对特定任务的单模态意图识别。然⽽在 现实场景中，⼈类意图具有复杂性，需要综合语⾔、语调、表情和动作等信息进⾏推断。为此，本研究提出了⼀种基于注意⼒机制 的新型多模态融合⽅法，以解决现实世界多模态场景中的意图识别问题。该⽅法通过为每个模态特征分别采⽤⾃注意⼒机制，旨在 捕捉并整合不同模态间的⻓程依赖关系，⾃适应调整各模态信息的重要性，并提供更丰富的表征。通过在每种模态的数据中添加明 确的模态标识符，模型能够区分并有效融合不同模态的信息，从⽽增强整体理解和决策能⼒。鉴于⽂本信息在跨模态交互中的重要 性，采⽤了⼀种以交叉注意⼒机制为中⼼的多模态融合⽅法，其中⽂本为主要模态，其他模态辅助并引导交互。该⽅法旨在促进⽂ 本、视觉和听觉模态之间的互动。最后，在MIntRec和MIntRec2.0基准数据集上进⾏了多模态意图识别实验。结果显⽰，该模型在 准确性、精确度、召回率和F1分数⽅⾯优于现有的多模态学习⽅法，⽐当前最佳基线提⾼了0.1%到0.5%。 提出了⼀种基于注意⼒机制的多模态融合⽅法，主要包含以下模块： 多模态特征提取： ◦ ⽂本：使⽤BERT提取特征。 ◦ 视觉：使⽤SwinTransformer处理视频。 ◦ ⾳频：使⽤WavLM提取特征。 ◦ 模态对⻬与⾃注意⼒机制：捕捉单模态内的⻓距离依赖关系，并添加模态标识以区分不同模态。 多模态融合： ◦ 跨模态注意⼒机制：以⽂本模态为主导，引导⾮⽂本模态（⾳频、视觉）的交互。 ◦ 特征级注意⼒：动态调整各模态特征的权重，突出关键信息。 11. \* 2503.04201v1.pdf 《Knowledge-DecoupledSynergeticLearning:AnMLLMbasedCollaborativeApproachtoFew-shotMultimodal DialogueIntentionRecognition》 摘要：少样本多模态对话意图识别是电⼦商务领域的⼀个关键挑战。以往的⽅法主要通过后训练技术来增强模型分类能⼒。然⽽， 我们的分析表明，少样本多模态对话意图识别的训练涉及两个相互关联的任务，导致多任务学习中的跷跷板效应。这种现象归因于 训练过程中权重矩阵更新叠加产⽣的知识⼲扰。为了解决这些问题，我们提出了知识解耦协同学习（KDSL），通过使⽤较⼩的模型 将知识转化为可解释规则，同时应⽤较⼤模型的后训练。通过促进⼤型和⼩型多模态⼤语⾔模型之间的协作以进⾏预测，我们的⽅ 法展⽰了显著的改进。值得注意的是，我们在两个真实的淘宝数据集上取得了出⾊的结果，在线加权F1分数相⽐最先进⽅法分别提 ⾼了6.37%和6.28%，从⽽验证了我们框架的有效性。 12.  2502.06803v1.pdf 《EMOTIONRECOGNITIONANDGENERATION:ACOMPREHENSIVEREVIEWOFFACE,SPEECH,ANDTEXTMODALITIES》 系统综述了⼈⼯智能在情感识别（ER）和情感⽣成（EG）领域的最新进展，聚焦⾯部、语⾳和⽂本三种模态的交叉研究。论⽂指出 情感在⼈机交互中的重要性（如医疗、客服、教育等应⽤），但现有研究多碎⽚化，缺乏多模态整合分析。作者通过⽂献综述，梳 理了各模态的理论基础、技术⽅法（如CNN、LSTM、GAN、扩散模型等）、预处理技术、数据集和评估指标，并分析了SOTA模型 的性能。结果显⽰，深度学习显著提升了情感识别与⽣成的准确性和真实性，如FER中的EmoFAN、SEG中的PromptVC和TSG中的 GPT-4表现优异，但数据稀缺、泛化性不⾜和伦理问题（如深伪）仍需解决。 • 介绍了情感识别与⽣成的基本原理，包括基于Ekman等⼈提出的情绪分类模型（如愤怒、厌恶、恐惧等）。 • 将技术分为⾯部（FER/FEG）、语⾳（SER/SEG）和⽂本（TSR/TSG）三种模态，分别探讨其独特⽅法，如卷积神经⽹络 （CNN）、⻓短期记忆⽹络（LSTM）、基于注意⼒的模型（如Transformer）、⽣成对抗⽹络（GAN）和扩散模型。


### 第 6 页
13. \* 2503.19474v2.pdf 《A-MESS:AnchorbasedMultimodalEmbeddingwithSemanticSynchronizationforMultimodalIntentRecognition》 多模态意图识别（MultimodalIntentRecognition,MIR）旨在通过整合⽂本、视觉和⾳频等多种模态信息，识别⼈类意图。这在AI Agent应⽤中尤为重要，例如理解⽤⼾命令以执⾏任务。现有⽅法在捕捉模态间内在联系和意图语义表⽰⽅⾯存在不⾜，⾯临以下挑 战： 1. 挑战I:⽂本为中⼼，⾳频和视觉作为辅助信号，但这些信号中可能包含⼲扰信息，如何过滤噪声并保留关键信息？ 2. 挑战II:如何优化多模态联合表⽰，提⾼学习效率？ 提出了⼀种新框架A-MESS（Anchor-basedMultimodalEmbeddingwithSemanticSynchronization），通过以下⽅式解决 上述挑战： • Anchor-basedMultimodalEmbedding(A-ME)模块:选择关键“锚点”信息，融合多模态输⼊，减少冗余。 • SemanticSynchronization(SS)策略:结合⼤语⾔模型（LLM）⽣成的标签描述，通过三元组对⽐学习优化多模态表⽰与语义 空间的对⻬。 • 在两个MIR数据集（MIntRec和MIntRec2.0）上进⾏了⼴泛实验，取得了SOTA（State-of-the-Art）性能。 数据集 • MIntRec[4]:包含2224个样本，20个意图类别，分为训练（1334）、验证（445）、测试（445），涵盖⽂本、视频、⾳频模 态。 • MIntRec2.0[5]:更⼤规模，15000个样本，30个意图类别（9300个in-scope，5700个out-of-scope）。 14. \* s12911-024-02772-0.pdf 《Multimodalmachinelearningforlanguageandspeechmarkersidentificationinmentalhealth》研究了使⽤多模态机 器学习⽅法（结合⽂本和⾳频模态）来识别多种⼼理健康障碍的语⾔和语⾳标记，并将其与单模态⽅法（单独使⽤⽂本或⾳频）进 ⾏⽐较。研究设计了三种模型：单模态⽂本模型、单模态⾳频模型和多模态模型，并使⽤四种机器学习算法（⽀持向量机SVM、随 机森林RF、逻辑回归LogReg、完全连接神经⽹络FCNN）进⾏训练和评估。评估指标包括准确率、AUC-ROC得分、正类F1得分 （F1-1s）和负类F1得分（F1-0s）。 数据集：研究使⽤DAIC-WOZ数据集，包含189个临床访谈（⾳频、⽂本和视频模态），主要针对抑郁症、创伤后应激障碍 （PTSD）和焦虑症的诊断。数据集中的访谈由虚拟访谈者“Ellie”与真实参与者进⾏，平均时⻓16分钟。 标记识别：通过⽂献综述，研究团队整理了适⽤于多种⼼理健康障碍的语⾔和语⾳标记（⻅表1和表2）。例如： • 语⾔标记：抑郁症与负⾯情绪词、⾃我关注语⾔相关；精神分裂症与语⾔不连贯、宗教词汇相关。 • 语⾳标记：抑郁症与语调单调、语⾳强度降低相关；焦虑症与抖动（jitter）和闪光（shimmer）增加相关。 多模态融合：采⽤早期融合（earlyfusion）策略，将⽂本和⾳频特征合并为单⼀特征向量，输⼊到机器学习模型中。 结果分析 • ⽂本模型的优越性归因于LIWC特征与⼼理健康标记的强相关性，⽽⾳频模型受限于特征⼯程和标签偏⻅。 • 多模态模型通过早期融合利⽤了两者的优势，尤其在正类预测（F1-1s）上表现更好，表明⾳频特征（如⾳⾼、抖动）提供了⽂ 本⽆法捕捉的信息。 • 特征集“20⽂本+10⾳频”的优异表现表明，适当偏向⽂本特征有助于模型泛化。 15. \* MGC-Amodalmappingcouplingandgate-drivencontrastivelearningapproachfor multimodalintentrecognition.pdf 《MGC:AModalMappingCouplingandGate-DrivenContrastiveLearningApproachforMultimodalIntent Recognition》提出了⼀种基于模态映射耦合和⻔控驱动的对⽐学习策略（MGC），⽤于多模态意图识别，处理⽂本、⾳频和视频 模态。 多模态数据处理： • 模态映射耦合：通过注意⼒机制捕捉⽂本、⾳频和视觉信息的深层关联，解决模态间不⼀致性问题。 • ⻔控融合：引⼊⾃适应⻔控控制器，动态调整各模态的融合权重，优化特征表⽰。


### 第 7 页
• 对⽐学习：使⽤InfoNCE损失函数增强⽂本与融合特征的语义相似性，提升意图识别的准确性。 • 特征提取：使⽤BERT编码器提取⽂本特征，结合预训练模型处理⾳频和视觉特征 16. \*《AnEffectiveMultimodalRepresentationandFusionMethodforMultimodalIntent Recognition》 意图识别是⾃然语⾔理解中的⼀个关键任务。⽬前的研究主要集中在特定任务的单模态意图识别上。然⽽，在现实场景中，⼈类的 意图是复杂的，需要通过整合语⾳、语⽓、表情和动作等信息来判断。因此，本⽂提出了⼀种有效的多模态表⽰和融合⽅法 （EMRFM），⽤于在现实多模态场景中的意图识别。⾸先，基于预训练的BERT、Wav2vec2.0和FasterR-CNN提取⽂本、⾳频和视 觉特征。然后，考虑到各模态之间的互补性和⼀致性，构建了模态共享和模态特定编码器，以学习各模态的共享和特定特征表⽰。 最后，采⽤⾃适应基于注意⼒⻔控神经⽹络的多模态融合⽅法旨在消除噪声特征。我们在多模态意图识别MIntRec基准数据集上进 ⾏了全⾯实验。所提出的模型在准确性、精确度、召回率和F1分数⽅⾯均优于现有的多模态学习⽅法。此外，实验表明该模型的多 模态表⽰能够很好地学习到模态的共享和特定特征。模型的多模态融合实现了⾃适应融合，有效减少了可能的噪声⼲扰。 EMRFM框架包含四个主要模块： 1. 特征提取 ◦ ⽂本:使⽤预训练BERT提取语义特征。 ◦ ⾳频:使⽤Wav2vec2.0提取语⾳特征。 ◦ 视觉:使⽤FasterR-CNN检测说话⼈区域并提取特征。 2. 多模态表⽰学习 ◦ 共享特征编码器（Modality-sharedEncoder）:学习跨模态的共性特征（如共同的情感或⽬标），通过CMD损失（Central MomentDiscrepancy）对⻬不同模态的共享特征分布。 ◦ 特定特征编码器（Modality-specificEncoder）:学习各模态的独有特征（如⽂本的语义、语⾳的语调、视觉的表情），通 过正交约束（L2-norm）确保特定特征与共享特征的差异性。 ◦ 重构损失（ReconstructionLoss）:确保共享与特定特征能还原原始特征空间。 3. 多模态融合 ◦ 模态内融合:使⽤Self-Attention融合各模态的共享与特定特征。 ◦ 跨模态融合:通过Cross-Attention计算⽂本-视觉、⽂本-⾳频的关联特征，输⼊⻔控神经⽹络⽣成融合权重，⾃适应加权融合 （⽂本为主，视觉/⾳频为辅）。 数据集：MIntRec（⾸个真实场景多模态意图识别基准数据集，含2,224个样本，20类细粒度意图）。 17. \*《Human-in-the-LoopMultimodalIntentDetection》 来源:DigitalCommons,KennesawStateUniversity,2025 链接:https://digitalcommons.kennesaw.edu/[](https://digitalcommons.kennesaw.edu/masterstheses/54/) 提出了⼀种统⼀框架MMIU，⽤于多模态意图分类和分布外（OOD）检测，融合语⾳、⽂本、⼿势和⾯部表情数据。 ◦ 多模态数据处理： ▪ 伪OOD样本⽣成：通过凸混合分布内数据，⽣成伪分布外样本，增强模型对未知意图的检测能⼒。 ▪ 双层表⽰学习：在模态级别和语义级别学习多模态表⽰，解决语义差距问题。 ▪ 特征融合：使⽤注意⼒机制整合异构数据流，减少计算复杂性。


## 图片

![意图识别论文集 图片 1](images/意图识别论文集/意图识别论文集_image_1.png)

![意图识别论文集 图片 2](images/意图识别论文集/意图识别论文集_image_2.png)

![意图识别论文集 图片 3](images/意图识别论文集/意图识别论文集_image_3.png)


## 参考文献

此 PDF 未提供具体参考文献。
